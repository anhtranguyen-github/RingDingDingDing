{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhtranguyen-github/RingDingDingDing/blob/main/Simple_RAG_with_GROQ_Qdrant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSVGmeizGMLv"
      },
      "source": [
        "#Bài toán:\n",
        "Ứng dụng mô hình ngôn ngữ lớn để tra cứu và hỏi đáp về tài liệu các môn chuyên ngành CNTT.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpCPrT5z1jmg"
      },
      "source": [
        "### Giới thiệu\n",
        "\n",
        "Bài tập lớn này đề xuất ứng dụng mô hình ngôn ngữ lớn (LLM) và RAG (Retrieval Augmented Generation) để xây dựng hệ thống tra cứu và hỏi đáp về tài liệu các môn chuyên ngành CNTT. Hệ thống sẽ hỗ trợ người dùng tìm kiếm thông tin, giải đáp thắc mắc và học tập hiệu quả hơn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lrv6HHe3Q6c"
      },
      "source": [
        "### Pipeline:\n",
        "\n",
        "\n",
        "![A simple RAG system](https://research.aimultiple.com/wp-content/uploads/2023/09/RAG-Architect-612x406.png.webp)\n",
        "\n",
        "Nguồn ảnh: https://research.aimultiple.com/retrieval-augmented-generation/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Qu5JvNLFIK"
      },
      "source": [
        "Retrieval Augmented Generation (RAG) là một phương pháp được giới thiệu bởi các nhà nghiên cứu của Meta AI để giải quyết các task yêu cầu nhiều kiến thức (knowledge-intensive). RAG là kết hợp của thành phần truy xuất thông tin (Retrieval) với mô hình tạo sinh văn bản (Generation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJwyqO6NLOX8"
      },
      "source": [
        "Các tài liệu, kiến thức từ một nguồn (ví dụ: Wikipedia, Google drive, vv.) được embed bằng Embedding model và index vào Vector Database để phục vụ cho truy vấn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlPuqHUaLcS0"
      },
      "source": [
        "RAG lấy input đầu vào và dùng nó để truy xuất ra một tập hợp các tài liệu có liên quan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZBv2QA_LeBb"
      },
      "source": [
        "Sau đó, các tài liệu được thêm vào prompt dưới dạng in-context learning và được đưa vào generation model để tạo ra phản hồi.\n",
        "Một prompt ví dụ:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgdisKS8Lhy_"
      },
      "source": [
        "\"\"\"Use the following pieces of context to answer the question at the end.\n",
        "\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSbnMRwTLvQ6"
      },
      "source": [
        "#Cài đặt các thư viện cần thiết"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V40CiUVMDQkf",
        "outputId": "fb59bf3d-77db-4cb3-fa39-8279f23d3bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.40.0\n",
        "!pip install -q accelerate==0.29.3\n",
        "!pip install -q huggingface-hub==0.22.2\n",
        "!pip install -q auto-gptq==0.7.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYs9bl5yhEzd"
      },
      "source": [
        "\n",
        "#Chunk Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlwT15M54mPs"
      },
      "source": [
        "Vì các tài liệu (Document) dài, không thể cùng lưu trữ dưới dạng 1 vector chung mà cần phải chia nhỏ thành các cụm câu (Chunker) để lưu trữ và truy vấn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7AhYH03hEa3"
      },
      "outputs": [],
      "source": [
        "class Chunk:\n",
        "    def __init__(\n",
        "        self,\n",
        "        text: str = \"\",\n",
        "        doc_name: str = \"\",\n",
        "        doc_type: str = \"\",\n",
        "        doc_uuid: str = \"\",\n",
        "        chunk_id: str = \"\",\n",
        "    ):\n",
        "        self._text = text\n",
        "        self._doc_name = doc_name\n",
        "        self._doc_type = doc_type\n",
        "        self._doc_uuid = doc_uuid\n",
        "        self._chunk_id = chunk_id\n",
        "        self._tokens = 0\n",
        "        self._vector = None\n",
        "        self._score = 0\n",
        "\n",
        "    @property\n",
        "    def text(self):\n",
        "        return self._text\n",
        "\n",
        "    @property\n",
        "    def text_no_overlap(self):\n",
        "        return self._text_no_overlap\n",
        "\n",
        "    @property\n",
        "    def doc_name(self):\n",
        "        return self._doc_name\n",
        "\n",
        "    @property\n",
        "    def doc_type(self):\n",
        "        return self._doc_type\n",
        "\n",
        "    @property\n",
        "    def doc_uuid(self):\n",
        "        return self._doc_uuid\n",
        "\n",
        "    @property\n",
        "    def chunk_id(self):\n",
        "        return self._chunk_id\n",
        "\n",
        "    @property\n",
        "    def tokens(self):\n",
        "        return self._tokens\n",
        "\n",
        "    @property\n",
        "    def vector(self):\n",
        "        return self._vector\n",
        "\n",
        "    @property\n",
        "    def score(self):\n",
        "        return self._score\n",
        "\n",
        "    def set_uuid(self, uuid):\n",
        "        self._doc_uuid = uuid\n",
        "\n",
        "    def set_tokens(self, token):\n",
        "        self._tokens = token\n",
        "\n",
        "    def set_vector(self, vector):\n",
        "        self._vector = vector\n",
        "\n",
        "    def set_score(self, score):\n",
        "        self._score = score\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        \"\"\"Convert the Chunk object to a dictionary.\"\"\"\n",
        "        return {\n",
        "            \"text\": self.text,\n",
        "            \"doc_name\": self.doc_name,\n",
        "            \"doc_type\": self.doc_type,\n",
        "            \"doc_uuid\": self.doc_uuid,\n",
        "            \"chunk_id\": self.chunk_id,\n",
        "            \"tokens\": self.tokens,\n",
        "            \"vector\": self.vector,\n",
        "            \"score\": self.score,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: dict):\n",
        "        \"\"\"Construct a Chunk object from a dictionary.\"\"\"\n",
        "        chunk = cls(\n",
        "            text=data.get(\"text\", \"\"),\n",
        "            doc_name=data.get(\"doc_name\", \"\"),\n",
        "            doc_type=data.get(\"doc_type\", \"\"),\n",
        "            doc_uuid=data.get(\"doc_uuid\", \"\"),\n",
        "            chunk_id=data.get(\"chunk_id\", \"\"),\n",
        "        )\n",
        "        chunk.set_tokens(data.get(\"tokens\", 0))\n",
        "        chunk.set_vector(data.get(\"vector\", None))\n",
        "        chunk.set_score(data.get(\"score\", 0))\n",
        "        return chunk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hiOH9EEFHKn"
      },
      "source": [
        "#Document class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgHJQjP_MPdD"
      },
      "source": [
        "Lớp class cho mọi loại tài liệu được đọc và được lưu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6sOq56VFObM"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Document:\n",
        "    def __init__(\n",
        "        self,\n",
        "        text: str = \"\",\n",
        "        type: str = \"\",\n",
        "        name: str = \"\",\n",
        "        path: str = \"\",\n",
        "        link: str = \"\",\n",
        "        timestamp: str = \"\",\n",
        "        reader: str = \"\",\n",
        "        meta: dict = None,\n",
        "    ):\n",
        "        if meta is None:\n",
        "            meta = {}\n",
        "        self._text = text\n",
        "        self._type = type\n",
        "        self._name = name\n",
        "        self._path = path\n",
        "        self._link = link\n",
        "        self._timestamp = timestamp\n",
        "        self._reader = reader\n",
        "        self._meta = meta\n",
        "        self.chunks: list[Chunk] = []\n",
        "\n",
        "    @property\n",
        "    def text(self):\n",
        "        return self._text\n",
        "\n",
        "    @property\n",
        "    def type(self):\n",
        "        return self._type\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return self._name\n",
        "\n",
        "    @property\n",
        "    def path(self):\n",
        "        return self._path\n",
        "\n",
        "    @property\n",
        "    def link(self):\n",
        "        return self._link\n",
        "\n",
        "    @property\n",
        "    def timestamp(self):\n",
        "        return self._timestamp\n",
        "\n",
        "    @property\n",
        "    def reader(self):\n",
        "        return self._reader\n",
        "\n",
        "    @property\n",
        "    def meta(self):\n",
        "        return self._meta\n",
        "\n",
        "    @staticmethod\n",
        "    def to_json(document) -> dict:\n",
        "        \"\"\"Convert the Document object to a JSON dict.\"\"\"\n",
        "        doc_dict = {\n",
        "            \"text\": document.text,\n",
        "            \"type\": document.type,\n",
        "            \"name\": document.name,\n",
        "            \"path\": document.path,\n",
        "            \"link\": document.link,\n",
        "            \"timestamp\": document.timestamp,\n",
        "            \"reader\": document.reader,\n",
        "            \"meta\": document.meta,\n",
        "            \"chunks\": [chunk.to_dict() for chunk in document.chunks],\n",
        "        }\n",
        "        return doc_dict\n",
        "\n",
        "    @staticmethod\n",
        "    def from_json(doc_dict: dict):\n",
        "        \"\"\"Convert a JSON string to a Document object.\"\"\"\n",
        "        document = Document(\n",
        "            text=doc_dict.get(\"text\", \"\"),\n",
        "            type=doc_dict.get(\"type\", \"\"),\n",
        "            name=doc_dict.get(\"name\", \"\"),\n",
        "            path=doc_dict.get(\"path\", \"\"),\n",
        "            link=doc_dict.get(\"link\", \"\"),\n",
        "            timestamp=doc_dict.get(\"timestamp\", \"\"),\n",
        "            reader=doc_dict.get(\"reader\", \"\"),\n",
        "            meta=doc_dict.get(\"meta\", {}),\n",
        "        )\n",
        "        # Assuming Chunk has a from_dict method\n",
        "        document.chunks = [\n",
        "            Chunk.from_dict(chunk_data) for chunk_data in doc_dict.get(\"chunks\", [])\n",
        "        ]\n",
        "        return document\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbLyqKboh_1f"
      },
      "source": [
        "#Reader base class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt9LGcy944QX"
      },
      "source": [
        "Lớp cơ bản cho reader (để đọc các loại tài liệu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLN85jD7iD2H"
      },
      "outputs": [],
      "source": [
        "class Reader():\n",
        "    \"\"\"\n",
        "    Interface for Readers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.file_types = []\n",
        "\n",
        "    def load(\n",
        "        bytes: list[str],\n",
        "        contents: list[str],\n",
        "        paths: list[str],\n",
        "        fileNames: list[str],\n",
        "        document_type: str,\n",
        "    ) -> list[Document]:\n",
        "        \"\"\"\n",
        "        @parameter: bytes : list[str] - List of bytes\n",
        "        @parameter: contents : list[str] - List of string content\n",
        "        @parameter: paths : list[str] - List of paths to files\n",
        "        @parameter: fileNames : list[str] - List of file names\n",
        "        @parameter: document_type : str - Document type\n",
        "        @returns list[Document] - Lists of documents.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"load method must be implemented by a subclass.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UavxBbbtiJeW"
      },
      "source": [
        "#Implement SimpleReader\n",
        "\n",
        "Sử dụng SimpleReader để đọc các văn bản dạng .txt, .md, .mdx, and .json\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1a1oU_AiOqF"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import glob\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "from wasabi import msg\n",
        "\n",
        "\n",
        "class SimpleReader(Reader):\n",
        "    \"\"\"\n",
        "    The SimpleReader reads .txt, .md, .mdx, and .json files. It can handle both paths, content and bytes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.file_types = [\".txt\", \".md\", \".mdx\", \".json\"]\n",
        "        self.name = \"SimpleReader\"\n",
        "        self.description = \"Reads text, markdown, and json files.\"\n",
        "\n",
        "    def load(\n",
        "        self,\n",
        "        bytes: list[str] = None,\n",
        "        contents: list[str] = None,\n",
        "        paths: list[str] = None,\n",
        "        fileNames: list[str] = None,\n",
        "        document_type: str = \"Documentation\",\n",
        "    ) -> list[Document]:\n",
        "        \"\"\"\n",
        "        @parameter: bytes : list[str] - List of bytes\n",
        "        @parameter: contents : list[str] - List of string content\n",
        "        @parameter: paths : list[str] - List of paths to files\n",
        "        @parameter: fileNames : list[str] - List of file names\n",
        "        @parameter: document_type : str - Document type\n",
        "        @returns list[Document] - Lists of documents.\n",
        "        \"\"\"\n",
        "        if fileNames is None:\n",
        "            fileNames = []\n",
        "        if paths is None:\n",
        "            paths = []\n",
        "        if contents is None:\n",
        "            contents = []\n",
        "        if bytes is None:\n",
        "            bytes = []\n",
        "        documents = []\n",
        "\n",
        "        # If paths exist\n",
        "        if len(paths) > 0:\n",
        "            for path in paths:\n",
        "                if path != \"\":\n",
        "                    data_path = Path(path)\n",
        "                    if data_path.exists():\n",
        "                        if data_path.is_file():\n",
        "                            documents += self.load_file(data_path, document_type)\n",
        "                        else:\n",
        "                            documents += self.load_directory(data_path, document_type)\n",
        "                    else:\n",
        "                        msg.warn(f\"Path {data_path} does not exist\")\n",
        "\n",
        "        # If bytes exist\n",
        "        if len(bytes) > 0 and len(bytes) == len(fileNames):\n",
        "            for byte, fileName in zip(bytes, fileNames):\n",
        "                decoded_bytes = base64.b64decode(byte)\n",
        "                try:\n",
        "                    original_text = decoded_bytes.decode(\"utf-8\")\n",
        "                except UnicodeDecodeError:\n",
        "                    msg.fail(\n",
        "                        f\"Error decoding text for file {fileName}. The file might not be a text file.\"\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "                if \".json\" in fileName:\n",
        "                    json_obj = json.loads(original_text)\n",
        "                    try:\n",
        "                        document = Document.from_json(json_obj)\n",
        "                    except Exception as e:\n",
        "                        raise Exception(f\"Loading JSON failed {e}\")\n",
        "\n",
        "                else:\n",
        "                    document = Document(\n",
        "                        name=fileName,\n",
        "                        text=original_text,\n",
        "                        type=document_type,\n",
        "                        timestamp=str(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")),\n",
        "                        reader=self.name,\n",
        "                    )\n",
        "                documents.append(document)\n",
        "\n",
        "        # If content exist\n",
        "        if len(contents) > 0 and len(contents) == len(fileNames):\n",
        "            for content, fileName in zip(contents, fileNames):\n",
        "                document = Document(\n",
        "                    name=fileName,\n",
        "                    text=content,\n",
        "                    type=document_type,\n",
        "                    timestamp=str(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")),\n",
        "                    reader=self.name,\n",
        "                )\n",
        "                documents.append(document)\n",
        "\n",
        "        msg.good(f\"Loaded {len(documents)} documents\")\n",
        "        return documents\n",
        "\n",
        "    def load_file(self, file_path: Path, document_type: str) -> list[Document]:\n",
        "        \"\"\"Loads text file\n",
        "        @param file_path : Path - Path to file\n",
        "        @param document_type : str - Document Type\n",
        "        @returns list[Document] - Lists of documents.\n",
        "        \"\"\"\n",
        "        documents = []\n",
        "\n",
        "        if file_path.suffix not in self.file_types:\n",
        "            msg.warn(f\"{file_path.suffix} not supported\")\n",
        "            return []\n",
        "\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "            msg.info(f\"Reading {str(file_path)}\")\n",
        "\n",
        "            if file_path.suffix == \".json\":\n",
        "                json_obj = json.loads(f.read())\n",
        "                try:\n",
        "                    document = Document.from_json(json_obj)\n",
        "                except Exception as e:\n",
        "                    raise Exception(f\"Loading JSON failed {e}\")\n",
        "\n",
        "            else:\n",
        "                document = Document(\n",
        "                    text=f.read(),\n",
        "                    type=document_type,\n",
        "                    name=str(file_path),\n",
        "                    link=str(file_path),\n",
        "                    timestamp=str(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")),\n",
        "                    reader=self.name,\n",
        "                )\n",
        "            documents.append(document)\n",
        "        msg.good(f\"Loaded {str(file_path)}\")\n",
        "        return documents\n",
        "\n",
        "    def load_directory(self, dir_path: Path, document_type: str) -> list[Document]:\n",
        "        \"\"\"Loads text files from a directory and its subdirectories.\n",
        "\n",
        "        @param dir_path : Path - Path to directory\n",
        "        @param document_type : str - Document Type\n",
        "        @returns list[Document] - List of documents\n",
        "        \"\"\"\n",
        "        # Initialize an empty dictionary to store the file contents\n",
        "        documents = []\n",
        "\n",
        "        # Convert dir_path to string, in case it's a Path object\n",
        "        dir_path_str = str(dir_path)\n",
        "\n",
        "        # Loop through each file type\n",
        "        for file_type in self.file_types:\n",
        "            # Use glob to find all the files in dir_path and its subdirectories matching the current file_type\n",
        "            files = glob.glob(f\"{dir_path_str}/**/*{file_type}\", recursive=True)\n",
        "\n",
        "            # Loop through each file\n",
        "            for file in files:\n",
        "                msg.info(f\"Reading {str(file)}\")\n",
        "                with open(file, encoding=\"utf-8\") as f:\n",
        "                    document = Document(\n",
        "                        text=f.read(),\n",
        "                        type=document_type,\n",
        "                        name=str(file),\n",
        "                        link=str(file),\n",
        "                        timestamp=str(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")),\n",
        "                        reader=self.name,\n",
        "                    )\n",
        "\n",
        "                    documents.append(document)\n",
        "\n",
        "        msg.good(f\"Loaded {len(documents)} documents\")\n",
        "        return documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGfHCl64jnJa"
      },
      "outputs": [],
      "source": [
        "reader = SimpleReader()\n",
        "path = \"/content/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhsx87_LqSFe",
        "outputId": "300f21e8-c8a4-4e4c-83e7-845fe7e717f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 1P-qUbqGflFpSp6joMzxSYnp4ZPZVwzX2 25-100.pdf\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P-qUbqGflFpSp6joMzxSYnp4ZPZVwzX2\n",
            "To: /content/data/25-100.pdf\n",
            "100% 799k/799k [00:00<00:00, 137MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "#import a folder from drive: https://drive.google.com/drive/u/0/folders/1ML3-o2iuPPmHsNWyxWBXTZt20qWL7xe9\n",
        "\n",
        "!gdown --folder 1ML3-o2iuPPmHsNWyxWBXTZt20qWL7xe9 -O /content/data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuRXnTO-LtrP"
      },
      "source": [
        "#Implement PDFReader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSvXTR_vMdj1"
      },
      "source": [
        "Sử dụng PyPDF2 để đọc file PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtJAweoZL3ya",
        "outputId": "86836417-34b8-4ec5-a529-e58ccdf342c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0h-dNbNLxfL"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    from PyPDF2 import PdfReader\n",
        "except Exception:\n",
        "    msg.warn(\"PyPDF2 not installed, your base installation might be corrupted.\")\n",
        "\n",
        "\n",
        "class PDFReader(Reader):\n",
        "    \"\"\"\n",
        "    The PDFReader reads .pdf files using Unstructured.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.file_types = [\".pdf\"]\n",
        "        self.requires_library = [\"PyPDF2\"]\n",
        "        self.name = \"PDFReader\"\n",
        "        self.description = \"Reads PDF files using the PyPDF2 library\"\n",
        "\n",
        "    def load(\n",
        "        self,\n",
        "        bytes: list[str] = None,\n",
        "        contents: list[str] = None,\n",
        "        paths: list[str] = None,\n",
        "        fileNames: list[str] = None,\n",
        "        document_type: str = \"Documentation\",\n",
        "    ) -> list[Document]:\n",
        "        \"\"\"Ingest data into Weaviate\n",
        "        @parameter: bytes : list[str] - List of bytes\n",
        "        @parameter: contents : list[str] - List of string content\n",
        "        @parameter: paths : list[str] - List of paths to files\n",
        "        @parameter: fileNames : list[str] - List of file names\n",
        "        @parameter: document_type : str - Document type\n",
        "        @returns list[Document] - Lists of documents.\n",
        "        \"\"\"\n",
        "        if fileNames is None:\n",
        "            fileNames = []\n",
        "        if paths is None:\n",
        "            paths = []\n",
        "        if contents is None:\n",
        "            contents = []\n",
        "        if bytes is None:\n",
        "            bytes = []\n",
        "        documents = []\n",
        "\n",
        "        # If paths exist\n",
        "        if len(paths) > 0:\n",
        "            for path in paths:\n",
        "                if path != \"\":\n",
        "                    data_path = Path(path)\n",
        "                    if data_path.exists():\n",
        "                        if data_path.is_file():\n",
        "                            documents += self.load_file(data_path, document_type)\n",
        "                        else:\n",
        "                            documents += self.load_directory(data_path, document_type)\n",
        "                    else:\n",
        "                        msg.warn(f\"Path {data_path} does not exist\")\n",
        "\n",
        "        # If bytes exist\n",
        "        if len(bytes) > 0 and len(bytes) == len(fileNames):\n",
        "            for byte, fileName in zip(bytes, fileNames):\n",
        "                decoded_bytes = base64.b64decode(byte)\n",
        "                with open(f\"{fileName}\", \"wb\") as file:\n",
        "                    file.write(decoded_bytes)\n",
        "\n",
        "                documents += self.load_file(f\"{fileName}\", document_type)\n",
        "                os.remove(f\"{fileName}\")\n",
        "\n",
        "        # If content exist\n",
        "        if len(contents) > 0 and len(contents) == len(fileNames):\n",
        "            for content, fileName in zip(contents, fileNames):\n",
        "                document = Document(\n",
        "                    name=fileName,\n",
        "                    text=content,\n",
        "                    type=document_type,\n",
        "                    timestamp=str(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")),\n",
        "                    reader=self.name,\n",
        "                )\n",
        "                documents.append(document)\n",
        "\n",
        "        msg.good(f\"Loaded {len(documents)} documents\")\n",
        "        return documents\n",
        "\n",
        "    def load_file(self, file_path: Path, document_type: str) -> list[Document]:\n",
        "        \"\"\"Loads .pdf file\n",
        "        @param file_path : Path - Path to file\n",
        "        @param document_type : str - Document Type\n",
        "        @returns list[Document] - Lists of documents.\n",
        "        \"\"\"\n",
        "        documents = []\n",
        "        full_text = \"\"\n",
        "        reader = PdfReader(file_path)\n",
        "\n",
        "        for page in reader.pages:\n",
        "            full_text += page.extract_text() + \"\\n\\n\"\n",
        "\n",
        "        document = Document(\n",
        "            text=full_text,\n",
        "            type=document_type,\n",
        "            name=str(file_path),\n",
        "            link=str(file_path),\n",
        "            timestamp=str(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")),\n",
        "            reader=self.name,\n",
        "        )\n",
        "        documents.append(document)\n",
        "        msg.good(f\"Loaded {str(file_path)}\")\n",
        "        return documents\n",
        "\n",
        "    def load_directory(self, dir_path: Path, document_type: str) -> list[Document]:\n",
        "        \"\"\"Loads .pdf files from a directory and its subdirectories.\n",
        "\n",
        "        @param dir_path : Path - Path to directory\n",
        "        @param document_type : str - Document Type\n",
        "        @returns list[Document] - List of documents\n",
        "        \"\"\"\n",
        "        # Initialize an empty dictionary to store the file contents\n",
        "        documents = []\n",
        "\n",
        "        # Convert dir_path to string, in case it's a Path object\n",
        "        dir_path_str = str(dir_path)\n",
        "\n",
        "        # Loop through each file type\n",
        "        for file_type in self.file_types:\n",
        "            # Use glob to find all the files in dir_path and its subdirectories matching the current file_type\n",
        "            files = glob.glob(f\"{dir_path_str}/**/*{file_type}\", recursive=True)\n",
        "\n",
        "            # Loop through each file\n",
        "            for file in files:\n",
        "                msg.info(f\"Reading {str(file)}\")\n",
        "                with open(file, encoding=\"utf-8\"):\n",
        "                    documents += self.load_file(file, document_type=document_type)\n",
        "\n",
        "        msg.good(f\"Loaded {len(documents)} documents\")\n",
        "        return documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xaFF4yyMXa7"
      },
      "outputs": [],
      "source": [
        "reader = PDFReader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xA3J_E_MhPv",
        "outputId": "12668f28-186c-4807-ee69-b320fe7f1c25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Reading /content/data/25-100.pdf\u001b[0m\n",
            "\u001b[38;5;2m✔ Loaded /content/data/25-100.pdf\u001b[0m\n",
            "\u001b[38;5;2m✔ Loaded 1 documents\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "documents = reader.load_directory(\n",
        "    dir_path = path,\n",
        "    document_type=\"document\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EHSL-AHo87f"
      },
      "source": [
        "#Chunker\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeCoi7Kl51WP"
      },
      "source": [
        "Lớp chunker sử dụng để chia nhỏ các document thành các chunks với tham số:\n",
        "\n",
        "1.   Units : số câu trong 1 chunk\n",
        "2.   Overlays: để tránh mất ngữ nghĩa trong một số trường hợp, overlays là số câu chung của 2 chunk liền kề.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJauehpYpK6N"
      },
      "outputs": [],
      "source": [
        "class Chunker():\n",
        "    \"\"\"\n",
        "    Interface for Chunking.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.default_units = 100\n",
        "        self.default_overlap = 50\n",
        "\n",
        "    def chunk(\n",
        "        self, documents: list[Document], units: int, overlap: int\n",
        "    ) -> list[Document]:\n",
        "        \"\"\"Chunk documents into chunks based on units and overlap.\n",
        "\n",
        "        @parameter: documents : list[Document] - List of documents\n",
        "        @parameter: units : int - How many units per chunk (words, sentences, etc.)\n",
        "        @parameter: overlap : int - How much overlap between the chunks\n",
        "        @returns list[str] - List of documents that contain the chunks.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"chunk method must be implemented by a subclass.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqR4lZUHpTBZ"
      },
      "source": [
        "#Implement Sentence chunker\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCYH5EzU59eb"
      },
      "source": [
        "Sử dụng Spacy để chia văn bản thành các tập các câu nhỏ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4H_hmcppvJ8"
      },
      "outputs": [],
      "source": [
        "import contextlib\n",
        "\n",
        "from tqdm import tqdm\n",
        "from wasabi import msg\n",
        "\n",
        "with contextlib.suppress(Exception):\n",
        "    import spacy\n",
        "\n",
        "\n",
        "class SentenceChunker(Chunker):\n",
        "    \"\"\"\n",
        "    SentenceChunker for built with spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"WordChunker\"\n",
        "        self.requires_library = [\"spacy\"]\n",
        "        self.default_units = 3\n",
        "        self.default_overlap = 2\n",
        "        self.description = \"Chunk documents by sentences. You can specify how many sentences should overlap between chunks to improve retrieval.\"\n",
        "        try:\n",
        "            self.nlp = spacy.blank(\"en\")\n",
        "            self.nlp.add_pipe(\"sentencizer\")\n",
        "            self.nlp.max_length = 3000000\n",
        "        except:\n",
        "            self.nlp = None\n",
        "\n",
        "    def chunk(\n",
        "        self, documents: list[Document], units: int, overlap: int\n",
        "    ) -> list[Document]:\n",
        "        \"\"\"Chunk documents into chunks based on units and overlap\n",
        "        @parameter: documents : list[Document] - List of documents\n",
        "        @parameter: units : int - How many units per chunk (words, sentences, etc.)\n",
        "        @parameter: overlap : int - How much overlap between the chunks\n",
        "        @returns list[str] - List of documents that contain the chunks.\n",
        "        \"\"\"\n",
        "        for document in tqdm(\n",
        "            documents, total=len(documents), desc=\"Chunking documents\"\n",
        "        ):\n",
        "            # Skip if document already contains chunks\n",
        "            if len(document.chunks) > 0:\n",
        "                continue\n",
        "\n",
        "            doc = list(self.nlp(document.text).sents)\n",
        "\n",
        "            if units > len(doc) or units < 1:\n",
        "                msg.warn(\n",
        "                    f\"Unit value either exceeds length of actual document or is below 1 ({units}/{len(doc)})\"\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            if overlap >= units:\n",
        "                msg.warn(\n",
        "                    f\"Overlap value is greater than unit (Units {units}/ Overlap {overlap})\"\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            i = 0\n",
        "            split_id_counter = 0\n",
        "            while i < len(doc):\n",
        "                # Overlap\n",
        "                start_i = i\n",
        "                end_i = i + units\n",
        "                if end_i > len(doc):\n",
        "                    end_i = len(doc)  # Adjust for the last chunk\n",
        "\n",
        "                text = \"\"\n",
        "                for sent in doc[start_i:end_i]:\n",
        "                    text += sent.text\n",
        "\n",
        "                doc_chunk = Chunk(\n",
        "                    text=text,\n",
        "                    doc_name=document.name,\n",
        "                    doc_type=document.type,\n",
        "                    chunk_id=split_id_counter,\n",
        "                )\n",
        "                document.chunks.append(doc_chunk)\n",
        "                split_id_counter += 1\n",
        "\n",
        "                # Exit loop if this was the last possible chunk\n",
        "                if end_i == len(doc):\n",
        "                    break\n",
        "\n",
        "                i += units - overlap  # Step forward, considering overlap\n",
        "\n",
        "        return documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjJrdX2fqAFe",
        "outputId": "4874c01c-e0d7-4d09-815e-3032e1b25cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Chunking documents: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n"
          ]
        }
      ],
      "source": [
        "chunker = SentenceChunker()\n",
        "\n",
        "\n",
        "units = 3 # Number of sentences per chunk\n",
        "overlap = 2 # Overlap between chunks\n",
        "\n",
        "\n",
        "chunked_documents = chunker.chunk(documents, units, overlap)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0MfCU1NyZoi",
        "outputId": "da31e851-346f-40c3-9d08-7a9eb09863ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1872\n"
          ]
        }
      ],
      "source": [
        "#print numbers of chunks in all docs\n",
        "\n",
        "for document in chunked_documents:\n",
        "    print(len(document.chunks))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-88WrS4K6nRG"
      },
      "source": [
        "#Embedder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAo6AiubNFqw"
      },
      "source": [
        "Lớp embedder chuyển các chunk thành các vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHqDRFRu8P3E"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import re\n",
        "\n",
        "def strip_non_letters(s: str):\n",
        "    return re.sub(r\"[^a-zA-Z0-9]\", \"_\", s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTOCrzP17lHy"
      },
      "outputs": [],
      "source": [
        "class Embedder():\n",
        "    \"\"\"\n",
        "    Interface forEmbedding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vectorizer = \"\"\n",
        "\n",
        "    def embed(documents: list[Document], batch_size: int = 100) -> bool:\n",
        "        \"\"\"Embed  documents and its chunks\n",
        "        @parameter: documents : list[Document] - List of  documents\n",
        "        @parameter: batch_size : int - Batch Size of Input\n",
        "        @returns bool - Bool whether the embedding what successful.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"embed method must be implemented by a subclass.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvsLpnPpELvG",
        "outputId": "abbfd9c8-b367-4827-af8a-fc2b4d9b6900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sentence-transformers/all-MiniLM-L6-v2'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 61 (delta 22), reused 54 (delta 19), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (61/61), 316.23 KiB | 6.08 MiB/s, done.\n",
            "Filtering content: 100% (5/5), 433.05 MiB | 52.94 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!mkdir sentence-transformers\n",
        "!git clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 sentence-transformers/all-MiniLM-L6-v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55ODxWh16pFh"
      },
      "outputs": [],
      "source": [
        "class MiniLMEmbedder(Embedder):\n",
        "    \"\"\"\n",
        "    MiniLMEmbedder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"MiniLMEmbedder\"\n",
        "        self.requires_library = [\"torch\", \"transformers\"]\n",
        "        self.description = \"Embeds and retrieves objects using SentenceTransformer's all-MiniLM-L6-v2 model\"\n",
        "        self.vectorizer = \"MiniLM\"\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        try:\n",
        "            import torch\n",
        "            from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "            def get_device():\n",
        "                if torch.cuda.is_available():\n",
        "                    return torch.device(\"cuda\")\n",
        "                elif torch.backends.mps.is_available():\n",
        "                    return torch.device(\"mps\")\n",
        "                else:\n",
        "                    return torch.device(\"cpu\")\n",
        "\n",
        "            self.device = get_device()\n",
        "\n",
        "            self.model = AutoModel.from_pretrained(\n",
        "                \"sentence-transformers/all-MiniLM-L6-v2\", device_map=self.device\n",
        "            )\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                \"sentence-transformers/all-MiniLM-L6-v2\", device_map=self.device\n",
        "            )\n",
        "            self.model = self.model.to(self.device)\n",
        "\n",
        "        except Exception as e:\n",
        "            msg.warn(str(e))\n",
        "            pass\n",
        "\n",
        "    def embed(\n",
        "        self,\n",
        "        documents: list[Document],\n",
        "    ) -> list[Document]:\n",
        "        \"\"\"Embed documents and its chunks\n",
        "        @parameter: documents : list[Document] - List of documents\n",
        "        @returns bool - Bool whether the embedding what successful.\n",
        "        \"\"\"\n",
        "        for document in tqdm(\n",
        "            documents, total=len(documents), desc=\"Vectorizing document chunks\"\n",
        "        ):\n",
        "            for chunk in document.chunks:\n",
        "                chunk.set_vector(self.vectorize_chunk(chunk.text))\n",
        "\n",
        "        return documents\n",
        "\n",
        "    def vectorize_chunk(self, chunk) -> list[float]:\n",
        "        try:\n",
        "            import torch\n",
        "\n",
        "            text = chunk\n",
        "            tokens = self.tokenizer.tokenize(text)\n",
        "\n",
        "            max_length = (\n",
        "                self.tokenizer.model_max_length\n",
        "            )  # Get the max sequence length for the model\n",
        "            batches = []\n",
        "            batch = []\n",
        "            token_count = 0\n",
        "\n",
        "            for token in tokens:\n",
        "                token_length = len(\n",
        "                    self.tokenizer.encode(token, add_special_tokens=False)\n",
        "                )\n",
        "                if token_count + token_length <= max_length:\n",
        "                    batch.append(token)\n",
        "                    token_count += token_length\n",
        "                else:\n",
        "                    batches.append(\" \".join(batch))\n",
        "                    batch = [token]\n",
        "                    token_count = token_length\n",
        "\n",
        "            # Don't forget to add the last batch\n",
        "            if batch:\n",
        "                batches.append(\" \".join(batch))\n",
        "\n",
        "            embeddings = []\n",
        "\n",
        "            for batch in batches:\n",
        "                inputs = self.tokenizer(\n",
        "                    batch, return_tensors=\"pt\", padding=True, truncation=True\n",
        "                )\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model(**inputs)\n",
        "                # Taking the mean of the hidden states to obtain an embedding for the batch\n",
        "                embedding = outputs.last_hidden_state.mean(dim=1)\n",
        "                embeddings.append(embedding)\n",
        "\n",
        "            # Concatenate the embeddings to make averaging easier\n",
        "            all_embeddings = torch.cat(embeddings)\n",
        "\n",
        "            averaged_embedding = all_embeddings.mean(dim=0)\n",
        "\n",
        "            averaged_embedding_list = averaged_embedding.tolist()\n",
        "\n",
        "            return averaged_embedding_list\n",
        "\n",
        "        except Exception:\n",
        "            raise\n",
        "\n",
        "    def vectorize_query(self, query: str) -> list[float]:\n",
        "        return self.vectorize_chunk(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odJS5By4_-9E",
        "outputId": "37bb93d0-a5b5-4004-bf4b-01ffff10ab4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n"
          ]
        }
      ],
      "source": [
        "# print tokenizer of embedder = MiniLMEmbedder()\n",
        "embedder = MiniLMEmbedder()\n",
        "\n",
        "print(embedder.tokenizer.model_max_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJMI7c3H932G",
        "outputId": "eb00a389-1ad1-45b6-c69e-a6c232d98544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.014248672872781754, 0.6279491782188416, 0.07291051745414734, -0.06212655082345009, 0.2749476730823517, -0.026274532079696655, 0.10822974890470505, 0.09660141170024872, 0.3085515797138214, -0.008498645387589931, 0.043142061680555344, 0.29481783509254456, -0.10924308747053146, -0.24091914296150208, -0.011748868972063065, 0.08396703004837036, 0.179739847779274, -0.32621002197265625, -0.13620297610759735, 0.02536865696310997, 0.1376171112060547, 0.5911679863929749, 0.012456698343157768, 0.18711793422698975, 0.24485571682453156, 0.6204167604446411, -0.3848669230937958, 0.2235054224729538, 0.1716795563697815, 0.05362347140908241, -0.07576702535152435, 0.0646846666932106, 0.548338770866394, 0.11030733585357666, -0.013388832099735737, 0.2673846483230591, -0.050195179879665375, 0.3121611177921295, 0.06551774591207504, 0.0342801995575428, 0.22141079604625702, -0.33449649810791016, 0.007368538063019514, 0.4206918179988861, 0.07220632582902908, 0.013380239717662334, -0.36111459136009216, 0.0195133239030838, 0.23831984400749207, -0.2285832017660141, -0.29591187834739685, -0.10434000939130783, -0.11239838600158691, 0.20610913634300232, 0.13785775005817413, 0.1450081616640091, 0.046975865960121155, -0.184494286775589, -0.09082244336605072, 0.13982783257961273, -0.1201745793223381, 0.27961671352386475, -0.09587389975786209, 0.1700647473335266, 0.3778621554374695, -0.07389473170042038, -0.1950337141752243, 0.035316791385412216, -0.4093726873397827, 0.0032847390975803137, 0.0994584858417511, 0.02214129827916622, -0.036910902708768845, 0.3944850564002991, -0.31364601850509644, -0.036606740206480026, -0.06013437733054161, -0.2229369431734085, 0.1368890106678009, 0.4247645139694214, -0.2764505445957184, -0.0915738120675087, 0.2689683437347412, 0.20246200263500214, -0.17788627743721008, 0.0922759547829628, 0.24276991188526154, -0.19790557026863098, -0.012291948311030865, 0.01030777022242546, -0.12089195847511292, -0.3409450650215149, 0.5429570078849792, 0.0624692440032959, -0.3585642874240875, 0.07029981166124344, -0.3954612910747528, -0.3174748420715332, 0.10707441717386246, 0.5687214732170105, 0.035127852112054825, 0.06466089934110641, 0.3314220905303955, 0.00816445890814066, -0.599833607673645, -0.5965390205383301, -0.04675664007663727, 0.17001089453697205, 0.17155294120311737, -0.07224047183990479, 0.05443726107478142, -0.3175937235355377, -0.4683338701725006, -0.18646866083145142, 0.16064037382602692, -0.2796321511268616, 0.05560135841369629, 0.0771183967590332, 0.14028488099575043, 0.39204269647598267, -0.0037099255714565516, -0.18330076336860657, -0.5675626993179321, 0.11233316361904144, 0.07918325811624527, -0.461041122674942, 0.3507329821586609, -3.439432699937481e-32, -0.15686435997486115, 0.1184937134385109, 0.016683420166373253, 0.4916893541812897, 0.0012337648076936603, 0.23491770029067993, -0.288961261510849, -0.09381178021430969, -0.1178063303232193, -0.1539045125246048, -0.14170102775096893, -0.4608234763145447, 0.23905055224895477, 0.18992500007152557, 0.17854031920433044, -0.39509186148643494, -0.35567131638526917, 0.6145386099815369, 0.13182277977466583, 0.04565266892313957, -0.32063189148902893, 0.09701579809188843, 0.1368212252855301, -0.13257943093776703, -0.20762306451797485, -0.19367413222789764, 0.037553269416093826, 0.01612681895494461, -0.12304655462503433, 0.0797848179936409, -0.17356868088245392, 0.15317031741142273, 0.13194540143013, -0.26212069392204285, 0.3049675226211548, -0.06851575523614883, -0.02301495335996151, -0.21034543216228485, -0.08788937330245972, 0.011024955660104752, -0.09034202247858047, -0.14436136186122894, 0.17658254504203796, -0.24794374406337738, -0.2590365409851074, 0.08860582113265991, -0.009940643794834614, -0.10384204983711243, -0.05181379243731499, 0.12834981083869934, 0.134820356965065, 0.21042662858963013, 0.519356369972229, -0.1484387218952179, 0.07941950112581253, -0.11147652566432953, 0.09487998485565186, 0.19784457981586456, 0.1596362292766571, 0.2995716333389282, -0.12412803620100021, 0.03878031671047211, 0.009845972061157227, 0.008317384868860245, 0.050555337220430374, 0.15548710525035858, -0.22867880761623383, -0.011120472103357315, 0.3708488643169403, -0.12972518801689148, -0.18457135558128357, -0.12040350586175919, 0.16469454765319824, 0.09294407814741135, -0.35679441690444946, -0.18429329991340637, 0.19048210978507996, -0.03226620703935623, -0.08767913281917572, 0.12673380970954895, -0.2934073209762573, -0.4395020306110382, 0.35340985655784607, -0.3438853323459625, -0.10621313005685806, 0.0891796126961708, 0.07440968602895737, -0.5466176271438599, 0.08010361343622208, 0.04069940000772476, -0.6786391139030457, -0.10485726594924927, 0.058154717087745667, -0.28283023834228516, 0.317564994096756, 2.3662664547854184e-32, -0.010187748819589615, -0.09741438925266266, -0.2990218698978424, -0.07664232701063156, 0.26675406098365784, 0.23646457493305206, 0.03905416652560234, 0.575631856918335, 0.14615154266357422, 0.36962786316871643, -0.1266850084066391, 0.37507379055023193, 0.10782547295093536, -0.1580837368965149, -0.29168668389320374, -0.02720589004456997, 0.5921658873558044, 0.009707157500088215, 0.3807494342327118, 0.16263897716999054, -0.10862813144922256, -0.29388120770454407, -0.2659408748149872, 0.2990720868110657, 0.3151107430458069, 0.26329001784324646, 0.0992472693324089, -0.07966634631156921, -0.03579990193247795, 0.09654941409826279, -0.14411073923110962, -0.46606722474098206, -0.19910086691379547, -0.21890635788440704, -0.31644847989082336, -0.13334523141384125, 0.2682477533817291, 0.06517894566059113, -0.09079024940729141, -0.09662099927663803, 0.26963579654693604, -0.04109825938940048, -0.06884897500276566, 0.18282467126846313, -0.13032692670822144, -0.5004522204399109, -0.059193745255470276, 0.05307799577713013, -0.08819808065891266, 0.3510536849498749, -0.018768053501844406, -0.09610778093338013, -0.07705815881490707, -0.011880348436534405, -0.35525792837142944, -0.0001162340267910622, -0.31511789560317993, 0.2346719354391098, -0.10782284289598465, -0.4358149766921997, -0.2942136526107788, 0.23750659823417664, -0.3021514415740967, 0.04882084205746651, 0.7021840214729309, -0.30925464630126953, 0.0034216642379760742, -0.04476138949394226, -0.23525813221931458, -0.0016942289657890797, 0.33714333176612854, -0.1571042388677597, -0.342934787273407, 0.06949952244758606, 0.2183643877506256, -0.06842229515314102, 0.10365822166204453, -0.11586517840623856, -0.17938077449798584, 0.25624558329582214, 0.4764675498008728, -0.198282390832901, 0.07858318090438843, -0.010312946513295174, 0.0810951516032219, 0.04027627408504486, -0.4451272189617157, 0.032699886709451675, -0.22880740463733673, 0.07487792521715164, -0.18179954588413239, 0.04659612104296684, 0.2856944501399994, 0.5337055921554565, -0.19787679612636566, -9.24579381944568e-08, 0.3153795003890991, -0.31093090772628784, -0.5043560862541199, -0.10278108716011047, 0.06307744234800339, 0.3162379264831543, 0.13003823161125183, -0.3666640818119049, -0.09281670302152634, 0.07269921153783798, 0.2723187506198883, 0.08991455286741257, -0.5265007019042969, -0.2451266497373581, 0.07961919903755188, 0.03309846296906471, 0.030720433220267296, -0.11204295605421066, -0.19987443089485168, 0.12912306189537048, 0.2312382310628891, 0.09002268314361572, -0.18637174367904663, -0.04016606882214546, -0.06027707830071449, 0.2739289104938507, -0.24838434159755707, 0.3319825232028961, -0.17472709715366364, -0.2719849646091461, 0.2525425851345062, 0.3528963327407837, -0.19360528886318207, -0.04215261712670326, 0.160451278090477, 0.14347076416015625, 0.2545398771762848, -0.07663055509328842, 0.3501338064670563, -0.11172684282064438, 0.1425132155418396, -0.06053195521235466, 0.31652048230171204, 0.09684863686561584, 0.09315149486064911, 0.09204339236021042, -0.1270512044429779, -0.18207870423793793, 0.022121453657746315, -0.18885520100593567, 0.13415168225765228, 0.06770988553762436, 0.16509424149990082, 0.015884865075349808, -0.05822920799255371, -0.26021626591682434, 0.16284246742725372, -0.0577896311879158, -0.2976769208908081, 0.07412191480398178, 0.20973525941371918, 0.3282323181629181, 0.27492013573646545, -0.08913473039865494]\n"
          ]
        }
      ],
      "source": [
        "# vectorize_chunk a chunk\n",
        "\n",
        "chunk = \"This is a chunk of text.\"\n",
        "embedding = embedder.vectorize_chunk(chunk)\n",
        "print(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedder.embed(chunked_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40oQRN0Kyiw8",
        "outputId": "26849074-71f7-4731-f8cb-3c3340124ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Vectorizing document chunks: 100%|██████████| 1/1 [00:16<00:00, 16.93s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.Document at 0x7ec9ef69dea0>]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJfdP1eV8-1C"
      },
      "source": [
        "#Install vector database\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGJ_P2JP6Wz7"
      },
      "source": [
        "Sử dụng Vector Database là cơ sở dữ liệu lưu trữ các vector sau khi được embedded. Qdrant là 1 open source vector database. Ở đây em sử dụng API miễn phí do Qdrant cung cấp ( có giới hạn lưu trữ) để kết nốt đến vector store lưu trữ vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMIeYDGB88AJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf8dec1-6458-4ad3-fc5d-7c42f946a806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.3/229.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.2/309.2 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-aiplatform 1.52.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-iam 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-language 2.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "proto-plus 1.23.0 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 5.27.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.0 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.27.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U -q qdrant_client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUey814K3LUn"
      },
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient, models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUBT2TkB3Hnr"
      },
      "outputs": [],
      "source": [
        "client = QdrantClient(\n",
        "    url=\"https://062652c5-bc12-45a8-9bd6-573590436438.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"pi-jVoM0EwsDSPk2sLc0zLFSYGboXVbLR06RqFlY-1wyztdzsHEoPw\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fwqfHCYkjKv"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    client.create_collection(\n",
        "        collection_name=\"Computer Network\",\n",
        "        vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE),\n",
        "    )\n",
        "except Exception as e:\n",
        "    msg.warn(str(e))\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRY6nvwpPVU6"
      },
      "source": [
        "Mỗi vector là các point, bước tiếp theo lưu các point vào vector database"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_point(chunk, vector):\n",
        "    return models.PointStruct(\n",
        "        id=chunk.chunk_id,\n",
        "        vector=vector,\n",
        "        payload={\"text\": chunk.text, \"doc_name\": chunk.doc_name, \"doc_type\": chunk.doc_type}\n",
        "    )\n",
        "\n",
        "points = []\n",
        "for document in chunked_documents:\n",
        "    for chunk in document.chunks:\n",
        "        vector = chunk.vector\n",
        "        point = create_point(chunk, vector)\n",
        "        points.append(point)\n",
        "\n",
        "try:\n",
        "    client.upsert(collection_name=\"Computer Network\", points=points)\n",
        "    print(\"Successfully inserted vectors into Qdrant\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to insert vectors: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEVMhakfx7xX",
        "outputId": "cc3e779c-1506-4132-f0c6-7fdfcf5c8093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully inserted vectors into Qdrant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is computer network\"\n",
        "query_vector = embedder.vectorize_query(query)\n",
        "\n",
        "# Search for similar vectors\n",
        "search_result = client.search(\n",
        "    collection_name=\"Computer Network\",\n",
        "    query_vector=query_vector,\n",
        "    limit=10,  # Number of results to retrieve\n",
        "    with_payload=True  # Retrieve the stored payload (metadata)\n",
        ")\n",
        "\n",
        "# Retrieve metadata\n",
        "metadata = []\n",
        "for result in search_result:\n",
        "    metadata.append(result.payload)\n",
        "\n",
        "# Process and display results\n",
        "for item in metadata:\n",
        "    print(f\"Document Name: {item['doc_name']}\")\n",
        "    print(f\"Document Type: {item['doc_type']}\")\n",
        "    print(f\"Text: {item['text']}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQgj3A_LImmX",
        "outputId": "4538fbf4-72f6-46d1-e248-3394f3be45ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Name: /content/data/25-100.pdf\n",
            "Document Type: document\n",
            "Text: \n",
            "Throughout the book we will use the term ‘‘computer network’’ to mean a col-\n",
            "lection of autonomous computers interconnected by a single technology.Two\n",
            "computers are said to be interconnected if they are able to exchange information.\n",
            "The connection need not be via a copper wire; fiber optics, microwaves, infrared,\n",
            "and communication satellites can also be used.\n",
            "\n",
            "Document Name: /content/data/25-100.pdf\n",
            "Document Type: document\n",
            "Text: The design and organization of\n",
            "these networks are the subjects of this book.\n",
            "Throughout the book we will use the term ‘‘computer network’’ to mean a col-\n",
            "lection of autonomous computers interconnected by a single technology.Two\n",
            "computers are said to be interconnected if they are able to exchange information.\n",
            "\n",
            "Document Name: /content/data/25-100.pdf\n",
            "Document Type: document\n",
            "Text: \n",
            "These systems are called computer networks .The design and organization of\n",
            "these networks are the subjects of this book.\n",
            "Throughout the book we will use the term ‘‘computer network’’ to mean a col-\n",
            "lection of autonomous computers interconnected by a single technology.\n",
            "\n",
            "Document Name: /content/data/25-100.pdf\n",
            "Document Type: document\n",
            "Text: \n",
            "A second goal of setting up a computer network has to do with people rather\n",
            "than information or even computers.A computer network can provide a powerfulcommunication medium among employees.Virtually every company that has\n",
            "two or more computers now has email (electronic mail ), which employees gener-\n",
            "ally use for a great deal of daily communication.\n",
            "\n",
            "Document Name: /content/data/25-100.pdf\n",
            "Document Type: document\n",
            "Text: The once-dominant concept of the‘‘computer center’’ as a room with a large computer to which users bring theirwork for processing is now totally obsolete (although data centers holding thou-sands of Internet servers are becoming common).The old model of a single com-puter serving all of the organization’s computational needs has been replaced by\n",
            "one in which a large number of separate but interconnected computers do the job.\n",
            "These systems are called computer networks .\n",
            "\n",
            "Document Name: /content/data/25-100.pdf\n",
            "Document Type: document\n",
            "Text: The client-server model involves requests and replies.\n",
            "A second goal of setting up a computer network has to do with people rather\n",
            "than information or even computers.A computer network can provide a powerfulcommunication medium among employees.\n",
            "\n",
            "Document Name: /content/data/25-100.pdf\n",
            "Document Type: document\n",
            "Text: Networks come in many sizes,\n",
            "shapes and forms, as we will see later.They are usually connected together to\n",
            "make larger networks, with the Internet being the most well-known example of a\n",
            "network of networks.\n",
            "There is considerable confusion in the literature between a computer network\n",
            "and a distributed system .\n",
            "\n",
            "Document Name: /content/data/25-100.pdf\n",
            "Document Type: document\n",
            "Text: For more information about distributed systems,see Tanenbaum and Van Steen (2007).\n",
            "1.1 USES OF COMPUTER NETWORKS\n",
            "Before we start to examine the technical issues in detail, it is worth devoting\n",
            "some time to pointing out why people are interested in computer networks andwhat they can be used for.After all, if nobody were interested in computer net-works, few of them would be built.\n",
            "\n",
            "Document Name: /content/data/25-100.pdf\n",
            "Document Type: document\n",
            "Text: The old model of a single com-puter serving all of the organization’s computational needs has been replaced by\n",
            "one in which a large number of separate but interconnected computers do the job.\n",
            "These systems are called computer networks .The design and organization of\n",
            "these networks are the subjects of this book.\n",
            "\n",
            "Document Name: /content/data/25-100.pdf\n",
            "Document Type: document\n",
            "Text: \n",
            "Computer networks make it very easy to communicate.They also make it\n",
            "easy for the people who run the network to snoop on the traffic.This sets up con-flicts over issues such as employee rights versus employer rights.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generator\n",
        "\n"
      ],
      "metadata": {
        "id": "2JEFB4CJLdXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index-llms-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGp64m0WLhqD",
        "outputId": "27d064a4-168e-44da-9f18-d3b3312fcb03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.1/324.1 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "llm = Groq(model=\"mixtral-8x7b-32768\", api_key=\"API KEY\")\n",
        "\n",
        "response = llm.complete(\"Explain the importance of low latency LLMs\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5pQyn9FLlfd",
        "outputId": "b0f6a273-36ae-4b6a-90b5-ccc009a23170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLMs, or low-latency messaging systems, are critical for applications that require real-time communication and data transfer. Low latency refers to the time it takes for a message to travel from the sender to the receiver, and low-latency LLMs aim to minimize this time as much as possible.\n",
            "\n",
            "The importance of low latency LLMs can be explained through the following points:\n",
            "\n",
            "1. Real-time communication: Low-latency LLMs enable real-time communication between applications, devices, and systems. This is critical for applications such as online gaming, financial trading, and industrial automation, where real-time data transfer is essential for optimal performance.\n",
            "2. Improved user experience: Low latency LLMs can significantly improve the user experience by reducing the time it takes for data to be transferred between applications and devices. This can lead to faster response times, smoother interactions, and a more enjoyable user experience.\n",
            "3. Increased efficiency: Low-latency LLMs can increase efficiency by reducing the time it takes for data to be transferred between applications and devices. This can lead to faster processing times, reduced downtime, and increased productivity.\n",
            "4. Competitive advantage: Low-latency LLMs can provide a competitive advantage for businesses by enabling faster data transfer and real-time communication. This can lead to quicker decision-making, improved customer service, and increased revenue.\n",
            "5. Reliability: Low-latency LLMs are designed to be highly reliable and can handle high volumes of data with minimal delay. This ensures that data is transferred accurately and efficiently, reducing the risk of errors and downtime.\n",
            "\n",
            "In summary, low latency LLMs are essential for applications that require real-time communication and data transfer. They can improve user experience, increase efficiency, provide a competitive advantage, and ensure reliability. As such, low latency LLMs are critical for a wide range of industries, from finance and gaming to manufacturing and healthcare.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prompted_user_question(user_question, metadata):\n",
        "    relevant_information = \"\" # Initialize an empty string\n",
        "\n",
        "    # Populate relevant_information with text from metadata\n",
        "    for item in metadata:\n",
        "        relevant_information += f\"* {item['text']}\\n\"\n",
        "\n",
        "    # Prepare the prompt for the LLM\n",
        "    prompt = f\"\"\"The user asked: \"{user_question}\".\n",
        "\n",
        "    Based on the information retrieved from the vector database, here are the most relevant pieces of information:\n",
        "    {relevant_information}\n",
        "    \"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "user_question = \"What is Computer network?\"\n",
        "\n",
        "# Call the function with the sample data\n",
        "prompt = prompted_user_question(user_question, metadata)\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J5s4ZDpNrHq",
        "outputId": "f65f1ae2-5935-4db4-b8e6-320422dd68ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The user asked: \"What is Computer network?\".\n",
            "\n",
            "    Based on the information retrieved from the vector database, here are the most relevant pieces of information:\n",
            "    * \n",
            "Throughout the book we will use the term ‘‘computer network’’ to mean a col-\n",
            "lection of autonomous computers interconnected by a single technology.Two\n",
            "computers are said to be interconnected if they are able to exchange information.\n",
            "The connection need not be via a copper wire; fiber optics, microwaves, infrared,\n",
            "and communication satellites can also be used.\n",
            "* The design and organization of\n",
            "these networks are the subjects of this book.\n",
            "Throughout the book we will use the term ‘‘computer network’’ to mean a col-\n",
            "lection of autonomous computers interconnected by a single technology.Two\n",
            "computers are said to be interconnected if they are able to exchange information.\n",
            "* \n",
            "These systems are called computer networks .The design and organization of\n",
            "these networks are the subjects of this book.\n",
            "Throughout the book we will use the term ‘‘computer network’’ to mean a col-\n",
            "lection of autonomous computers interconnected by a single technology.\n",
            "* \n",
            "A second goal of setting up a computer network has to do with people rather\n",
            "than information or even computers.A computer network can provide a powerfulcommunication medium among employees.Virtually every company that has\n",
            "two or more computers now has email (electronic mail ), which employees gener-\n",
            "ally use for a great deal of daily communication.\n",
            "* The once-dominant concept of the‘‘computer center’’ as a room with a large computer to which users bring theirwork for processing is now totally obsolete (although data centers holding thou-sands of Internet servers are becoming common).The old model of a single com-puter serving all of the organization’s computational needs has been replaced by\n",
            "one in which a large number of separate but interconnected computers do the job.\n",
            "These systems are called computer networks .\n",
            "* The client-server model involves requests and replies.\n",
            "A second goal of setting up a computer network has to do with people rather\n",
            "than information or even computers.A computer network can provide a powerfulcommunication medium among employees.\n",
            "* Networks come in many sizes,\n",
            "shapes and forms, as we will see later.They are usually connected together to\n",
            "make larger networks, with the Internet being the most well-known example of a\n",
            "network of networks.\n",
            "There is considerable confusion in the literature between a computer network\n",
            "and a distributed system .\n",
            "* For more information about distributed systems,see Tanenbaum and Van Steen (2007).\n",
            "1.1 USES OF COMPUTER NETWORKS\n",
            "Before we start to examine the technical issues in detail, it is worth devoting\n",
            "some time to pointing out why people are interested in computer networks andwhat they can be used for.After all, if nobody were interested in computer net-works, few of them would be built.\n",
            "* The old model of a single com-puter serving all of the organization’s computational needs has been replaced by\n",
            "one in which a large number of separate but interconnected computers do the job.\n",
            "These systems are called computer networks .The design and organization of\n",
            "these networks are the subjects of this book.\n",
            "* \n",
            "Computer networks make it very easy to communicate.They also make it\n",
            "easy for the people who run the network to snoop on the traffic.This sets up con-flicts over issues such as employee rights versus employer rights.\n",
            "\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "llm = Groq(model=\"mixtral-8x7b-32768\", api_key=\"API KEY\")\n",
        "\n",
        "response = llm.complete(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAOFwD4cOorW",
        "outputId": "60483099-6832-429e-ca74-c9856901ef93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A computer network is a collection of autonomous computers that are interconnected using a single technology, allowing them to exchange information. This interconnection can occur through various means, such as copper wire, fiber optics, microwaves, infrared, and communication satellites. The design and organization of these networks are the focus of this book.\n",
            "\n",
            "Computer networks can serve various purposes, including facilitating communication among employees and enabling the sharing of information and resources. They can also replace the traditional \"computer center\" model, where a large computer in a single room handles all of an organization's computational needs. Instead, a network of separate but interconnected computers can perform these tasks.\n",
            "\n",
            "Networks can come in many sizes, shapes, and forms and can be connected to create larger networks, with the Internet being the most well-known example. While computer networks offer many benefits, such as easy communication and resource sharing, they can also raise issues related to privacy and security.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kết luận\n",
        "1. Khả năng thích ứng cao:\n",
        "\n",
        "RAG có thể cập nhật kiến thức mới một cách dễ dàng mà không cần đào tạo lại mô hình LLM hoàn toàn, giúp theo kịp sự thay đổi của thông tin và dữ liệu theo thời gian.\n",
        "Nhờ vậy, RAG có thể cung cấp kết quả chính xác và đáng tin cậy hơn so với các mô hình truyền thống.\n",
        "2. Tăng độ minh bạch:\n",
        "\n",
        "RAG cho phép truy cập nguồn thông tin mà LLM sử dụng để tạo ra kết quả, giúp người dùng dễ dàng kiểm tra tính chính xác và độ tin cậy của thông tin.\n",
        "Điều này đặc biệt quan trọng trong các lĩnh vực như pháp lý, nơi độ chính xác của thông tin là rất quan trọng.\n",
        "3. Nâng cao khả năng giải thích:\n",
        "\n",
        "Việc hiểu được nguồn gốc của thông tin giúp người dùng hiểu rõ hơn về cách thức LLM đưa ra kết luận, từ đó có thể đưa ra đánh giá và quyết định sáng suốt hơn.\n",
        "Khả năng giải thích này cũng giúp ích cho việc phát triển và cải tiến các mô hình LLM trong tương lai.\n",
        "4. Tiết kiệm chi phí:\n",
        "\n",
        "Việc cập nhật kiến thức mới thông qua RAG thường tiết kiệm chi phí hơn so với việc đào tạo lại mô hình LLM hoàn toàn.\n",
        "Điều này giúp cho các ứng dụng AI tạo sinh trở nên dễ tiếp cận hơn với nhiều người dùng và doanh nghiệp."
      ],
      "metadata": {
        "id": "_Egmc_6laUbs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w5aMgue_ann8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPasrBZzfZIE4U8vyx9x4BF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}